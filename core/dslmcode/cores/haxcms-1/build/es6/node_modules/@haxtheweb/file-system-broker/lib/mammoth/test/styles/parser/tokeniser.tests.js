var e=require("hamjest"),s=e.assertThat,n=e.contains,o=e.hasProperties,i=require("../../../lib/styles/parser/tokeniser").tokenise,t=require("../../test")(module);function assertTokens(e,o){s(i(e),n.apply(null,o.concat([isToken("end",null)])))}function isToken(e,s){return o({name:e,value:s})}t("unknown tokens are tokenised",(function(){assertTokens("~",[isToken("unrecognisedCharacter","~")])})),t("empty string is tokenised to end of file token",(function(){assertTokens("",[])})),t("whitespace is tokenised",(function(){assertTokens(" \t\t  ",[isToken("whitespace")])})),t("identifiers are tokenised",(function(){assertTokens("Overture",[isToken("identifier","Overture")])})),t("integers are tokenised",(function(){assertTokens("123",[isToken("integer","123")])})),t("strings are tokenised",(function(){assertTokens("'Tristan'",[isToken("string","Tristan")])})),t("unterminated strings are tokenised",(function(){assertTokens("'Tristan",[isToken("unterminated-string","Tristan")])})),t("arrows are tokenised",(function(){assertTokens("=>",[isToken("arrow")])})),t("classes are tokenised",(function(){assertTokens(".overture",[isToken("dot"),isToken("identifier","overture")])})),t("colons are tokenised",(function(){assertTokens("::",[isToken("colon"),isToken("colon")])})),t("greater thans are tokenised",(function(){assertTokens(">>",[isToken("gt"),isToken("gt")])})),t("equals are tokenised",(function(){assertTokens("==",[isToken("equals"),isToken("equals")])})),t("startsWith symbols are tokenised",(function(){assertTokens("^=^=",[isToken("startsWith"),isToken("startsWith")])})),t("open parens are tokenised",(function(){assertTokens("((",[isToken("open-paren"),isToken("open-paren")])})),t("close parens are tokenised",(function(){assertTokens("))",[isToken("close-paren"),isToken("close-paren")])})),t("open square brackets are tokenised",(function(){assertTokens("[[",[isToken("open-square-bracket"),isToken("open-square-bracket")])})),t("close square brackets are tokenised",(function(){assertTokens("]]",[isToken("close-square-bracket"),isToken("close-square-bracket")])})),t("choices are tokenised",(function(){assertTokens("||",[isToken("choice"),isToken("choice")])})),t("can tokenise multiple tokens",(function(){assertTokens("The Magic Position",[isToken("identifier","The"),isToken("whitespace"),isToken("identifier","Magic"),isToken("whitespace"),isToken("identifier","Position")])}));